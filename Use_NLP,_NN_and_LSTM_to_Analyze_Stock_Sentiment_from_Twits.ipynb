{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4CdJLnNslaj"
      },
      "source": [
        "#Analyzing Stock Sentiment from Twits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Packages"
      ],
      "metadata": {
        "id": "XYkCDraTs20A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkwhXO9vslau"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import nltk\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raF1Sqw6slaw"
      },
      "source": [
        "## Introduction\n",
        "When deciding the value of a company, it's important to follow the news. For example, a product recall or natural disaster in a company's product chain. T his information can be turned into a signal by using Neural Network. \n",
        "\n",
        "In this project, the posts from the social media site [StockTwits](https://en.wikipedia.org/wiki/StockTwits) is used. The community on StockTwits is full of investors, traders, and entrepreneurs. Each message posted is a Twit. A model will be built to generate a sentiment score around these twits.\n",
        "\n",
        "A bunch of twits have been collected with the labeled sentiment of each. There are five scaled sentimets: very negative, negative, neutral, positive, very positive, from -2 to 2 in steps of 1 respectively. A sentiment analysis model will be built to learn to assign sentiment to twits on its own using this labeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Twits and Load Twits Data \n",
        "\n",
        "The fields represent the following:\n",
        "\n",
        "* `'message_body'`: The text of the twit.\n",
        "* `'sentiment'`: Sentiment score for the twit, ranges from -2 to 2 in steps of 1, with 0 being neutral.\n"
      ],
      "metadata": {
        "id": "VhCb4pFJwOZa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93RGtM4yslax",
        "outputId": "af32414d-2165-4127-908e-f2d20f939445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'message_body': '$FITB great buy at 26.00...ill wait', 'sentiment': 2, 'timestamp': '2018-07-01T00:00:09Z'}, {'message_body': '@StockTwits $MSFT', 'sentiment': 1, 'timestamp': '2018-07-01T00:00:42Z'}, {'message_body': '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', 'sentiment': 2, 'timestamp': '2018-07-01T00:01:24Z'}, {'message_body': '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', 'sentiment': 1, 'timestamp': '2018-07-01T00:01:47Z'}, {'message_body': '$AMD reveal yourself!', 'sentiment': 0, 'timestamp': '2018-07-01T00:02:13Z'}, {'message_body': '$AAPL Why the drop? I warren Buffet taking out his position?', 'sentiment': 1, 'timestamp': '2018-07-01T00:03:10Z'}, {'message_body': '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', 'sentiment': -2, 'timestamp': '2018-07-01T00:04:09Z'}, {'message_body': '$BAC ok good we&#39;re not dropping in price over the weekend, lol', 'sentiment': 1, 'timestamp': '2018-07-01T00:04:17Z'}, {'message_body': '$AMAT - Daily Chart, we need to get back to above 50.', 'sentiment': 2, 'timestamp': '2018-07-01T00:08:01Z'}, {'message_body': '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?', 'sentiment': -2, 'timestamp': '2018-07-01T00:09:03Z'}]\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'twits.json'), 'r') as f:\n",
        "    twits = json.load(f)\n",
        "\n",
        "print(twits['data'][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTb5r-EQslaz"
      },
      "source": [
        "### Length of Data is the number of twits in dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyZ_jPBdsla0",
        "outputId": "7a61ea16-1171-4191-f755-7f9275021dbc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1548010"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"print out the number of twits\"\"\"\n",
        "\n",
        "# TODO Implement \n",
        "len(twits['data'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeYf4q6ksla1"
      },
      "source": [
        "### Split Message Body and Sentiment Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYCPI0SZsla2",
        "outputId": "8af519b6-050e-4d59-9926-b15994caef83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['$FITB great buy at 26.00...ill wait', '@StockTwits $MSFT', '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating', '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.', '$AMD reveal yourself!', '$AAPL Why the drop? I warren Buffet taking out his position?', '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA', '$BAC ok good we&#39;re not dropping in price over the weekend, lol', '$AMAT - Daily Chart, we need to get back to above 50.', '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?']\n",
            "[4, 3, 4, 3, 2, 3, 0, 3, 4, 0]\n"
          ]
        }
      ],
      "source": [
        "messages = [twit['message_body'] for twit in twits['data']]\n",
        "# Since the sentiment scores are discrete, we'll scale the sentiments to 0 to 4 for use in our network\n",
        "sentiments = [twit['sentiment'] + 2 for twit in twits['data']]\n",
        "print(messages[:10])\n",
        "print(sentiments[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbIwH4lbsla2"
      },
      "source": [
        "## Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs8jHpH5sla4"
      },
      "source": [
        "### Remove unuseful symbols with regex using the re module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TiYytcisla4",
        "outputId": "26faa1eb-1dd7-43ed-ce31-1968a659ed06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def preprocess(message):\n",
        "    \"\"\"\n",
        "    This function takes a string as input, then performs these operations: \n",
        "        - lowercase\n",
        "        - remove URLs\n",
        "        - remove ticker symbols \n",
        "        - removes punctuation\n",
        "        - tokenize by splitting the string on whitespace \n",
        "        - removes any single character tokens\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "        message : The text message to be preprocessed.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "        tokens: The preprocessed text into tokens.\n",
        "    \"\"\" \n",
        "    #TODO: Implement \n",
        "    \n",
        "    # Lowercase the twit message\n",
        "    text = message.lower()\n",
        "    \n",
        "    # Replace URLs with a space in the message\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    \n",
        "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
        "    text = re.sub('\\$\\S+',' ',text)\n",
        "    \n",
        "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
        "    text = re.sub(r'@([A-Za-z0-9_]+)', ' ', text)\n",
        "\n",
        "    # Replace everything not a letter with a space\n",
        "    text = re.sub(r'[^a-zA-Z]',' ',text)\n",
        "    \n",
        "    # Tokenize by splitting the string on whitespace into a list of words\n",
        "    tokens = text.split(' ')\n",
        "\n",
        "    # Lemmatize words using the WordNetLemmatizer. You can ignore any word that is not longer than one character.\n",
        "    wnl = nltk.stem.WordNetLemmatizer()\n",
        "    tokens = [wnl.lemmatize(t) for t in tokens if len(t)>1]\n",
        "    \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKmiXjz_sla5"
      },
      "source": [
        "### Preprocess All the Twits "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlEg2urtsla9"
      },
      "outputs": [],
      "source": [
        "# TODO Implement\n",
        "tokenized = [preprocess(message) for message in messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKk7Nf8Ksla9"
      },
      "source": [
        "### Bag of Words\n",
        "With all messages tokenized, create a vocabulary and count up how often each word appears in the entire corpus. Use the [`Counter`](https://docs.python.org/3.1/library/collections.html#collections.Counter) function to count up all the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6rmnd0wsla-",
        "outputId": "cfc60007-d8da-4129-96a4-82c10e43df94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['$FITB great buy at 26.00...ill wait',\n",
              " '@StockTwits $MSFT',\n",
              " '#STAAnalystAlert for $TDG : Jefferies Maintains with a rating of Hold setting target price at USD 350.00. Our own verdict is Buy  http://www.stocktargetadvisor.com/toprating',\n",
              " '$AMD I heard there’s a guy who knows someone who thinks somebody knows something - on StockTwits.',\n",
              " '$AMD reveal yourself!',\n",
              " '$AAPL Why the drop? I warren Buffet taking out his position?',\n",
              " '$BA bears have 1 reason on 06-29 to pay more attention https://dividendbot.com?s=BA',\n",
              " '$BAC ok good we&#39;re not dropping in price over the weekend, lol',\n",
              " '$AMAT - Daily Chart, we need to get back to above 50.',\n",
              " '$GME 3% drop per week after spike... if no news in 3 months, back to 12s... if BO, then bingo... what is the odds?']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD7X2AoZsla-",
        "outputId": "246e0530-47d4-4a23-a497-f00b65f2a7dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['great', 'buy', 'at', 'ill', 'wait'],\n",
              " [],\n",
              " ['staanalystalert',\n",
              "  'for',\n",
              "  'jefferies',\n",
              "  'maintains',\n",
              "  'with',\n",
              "  'rating',\n",
              "  'of',\n",
              "  'hold',\n",
              "  'setting',\n",
              "  'target',\n",
              "  'price',\n",
              "  'at',\n",
              "  'usd',\n",
              "  'our',\n",
              "  'own',\n",
              "  'verdict',\n",
              "  'is',\n",
              "  'buy'],\n",
              " ['heard',\n",
              "  'there',\n",
              "  'guy',\n",
              "  'who',\n",
              "  'know',\n",
              "  'someone',\n",
              "  'who',\n",
              "  'think',\n",
              "  'somebody',\n",
              "  'know',\n",
              "  'something',\n",
              "  'on',\n",
              "  'stocktwits'],\n",
              " ['reveal', 'yourself'],\n",
              " ['why',\n",
              "  'the',\n",
              "  'drop',\n",
              "  'warren',\n",
              "  'buffet',\n",
              "  'taking',\n",
              "  'out',\n",
              "  'his',\n",
              "  'position'],\n",
              " ['bear', 'have', 'reason', 'on', 'to', 'pay', 'more', 'attention'],\n",
              " ['ok',\n",
              "  'good',\n",
              "  'we',\n",
              "  're',\n",
              "  'not',\n",
              "  'dropping',\n",
              "  'in',\n",
              "  'price',\n",
              "  'over',\n",
              "  'the',\n",
              "  'weekend',\n",
              "  'lol'],\n",
              " ['daily', 'chart', 'we', 'need', 'to', 'get', 'back', 'to', 'above'],\n",
              " ['drop',\n",
              "  'per',\n",
              "  'week',\n",
              "  'after',\n",
              "  'spike',\n",
              "  'if',\n",
              "  'no',\n",
              "  'news',\n",
              "  'in',\n",
              "  'month',\n",
              "  'back',\n",
              "  'to',\n",
              "  'if',\n",
              "  'bo',\n",
              "  'then',\n",
              "  'bingo',\n",
              "  'what',\n",
              "  'is',\n",
              "  'the',\n",
              "  'odds']]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxuvSBbcsla_",
        "outputId": "6cfe0d01-d114-474c-f65c-60b2a686a2c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "98376"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Create a vocabulary by using Bag of words\n",
        "\"\"\"\n",
        "\n",
        "# TODO: Implement \n",
        "all_words = []\n",
        "for w in tokenized:\n",
        "    for w1 in w:\n",
        "        all_words.append(w1)\n",
        "bow = Counter(all_words)\n",
        "len(bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx8YE0Mksla_"
      },
      "source": [
        "### Frequency of Words Appearing in Message\n",
        "With the vocabulary, we'll emove some of the most common words such as 'the', 'and', 'it', etc. These words don't contribute to identifying sentiment and are really common, resulting in a lot of noise in our input. If we can filter these out, then our network should have an easier time learning.\n",
        "\n",
        "We also want to remove really rare words that show up in a only a few twits. Here we want to divide the count of each word by the number of messages. Then remove words that only appear in some small fraction of the messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUnGtcZ9slbA",
        "outputId": "6a28ffbd-0935-431e-f447-c92262bec1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'the': 398754, 'to': 379487, 'is': 284865, 'for': 273537, 'on': 241663, 'of': 211334, 'and': 208471, 'in': 205307, 'this': 203542, 'it': 193484, 'at': 138453, 'will': 128180, 'up': 121567, 'are': 101424, 'you': 94278, 'that': 89655, 'be': 89277, 'short': 86642, 'what': 79115, 'today': 76240}\n",
            "47981\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Set the following variables:\n",
        "    freqs\n",
        "    low_cutoff\n",
        "    high_cutoff\n",
        "    K_most_common\n",
        "\"\"\"\n",
        "\n",
        "# TODO Implement \n",
        "\n",
        "# Dictionart that contains the Frequency of words appearing in messages.\n",
        "# The key is the token and the value is the frequency of that word in the corpus.\n",
        "total_appearing = len(tokenized)\n",
        "total_appearing\n",
        "freqs = {key:word_count/total_appearing for key, word_count in bow.items()}\n",
        "\n",
        "# Float that is the frequency cutoff. Drop words with a frequency that is lower or equal to this number.\n",
        "low_cutoff = 0.000001\n",
        "\n",
        "# Integer that is the cut off for most common words. Drop words that are the `high_cutoff` most common words.\n",
        "high_cutoff = 20\n",
        "\n",
        "# The k most common words in the corpus. Use `high_cutoff` as the k.\n",
        "#print(bow.most_common(high_cutoff))\n",
        "K_most_common = dict(bow.most_common(high_cutoff))\n",
        "\n",
        "filtered_words = [word for word in freqs if (freqs[word] > low_cutoff and word not in K_most_common)]\n",
        "print(K_most_common)\n",
        "print(len(filtered_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjeqPjkPslbA"
      },
      "source": [
        "### Updating Vocabulary by Removing Filtered Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWe59iM-slbB",
        "outputId": "c90fd85a-ee82-416b-8edb-50a971bb973c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1548010 [['great', 'buy', 'ill', 'wait'], [], ['staanalystalert', 'jefferies', 'maintains', 'with', 'rating', 'hold', 'setting', 'target', 'price', 'usd', 'our', 'own', 'verdict', 'buy'], ['heard', 'there', 'guy', 'who', 'know', 'someone', 'who', 'think', 'somebody', 'know', 'something', 'stocktwits'], ['reveal', 'yourself'], ['why', 'drop', 'warren', 'buffet', 'taking', 'out', 'his', 'position'], ['bear', 'have', 'reason', 'pay', 'more', 'attention'], ['ok', 'good', 'we', 're', 'not', 'dropping', 'price', 'over', 'weekend', 'lol'], ['daily', 'chart', 'we', 'need', 'get', 'back', 'above'], ['drop', 'per', 'week', 'after', 'spike', 'if', 'no', 'news', 'month', 'back', 'if', 'bo', 'then', 'bingo', 'odds']]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Set the following variables:\n",
        "    vocab\n",
        "    id2vocab\n",
        "    filtered\n",
        "\"\"\"\n",
        "#TODO Implement\n",
        "# A dictionary for the `filtered_words`. The key is the word and value is an id that represents the word. \n",
        "vocab = {w:i for i, w in enumerate(filtered_words)}\n",
        "# Reverse of the `vocab` dictionary. The key is word id and value is the word. \n",
        "id2vocab = {i:w for i, w in enumerate(filtered_words)}\n",
        "# tokenized with the words not in `filtered_words` removed.\n",
        "filtered = [[word for word in sentence if word in vocab] for sentence in tokenized]\n",
        "print(len(filtered), filtered[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQjLaBfIslbC"
      },
      "source": [
        "### Balancing the classes\n",
        "50% of the labeled twits are neutral. This means that the network will be 50% accurate just by guessing 0 every single time. To help the network learn appropriately, classes need to be balanced.\n",
        "That is, make sure each of the different sentiment scores show up roughly as frequently in the data.\n",
        "\n",
        "We'll go through each of the examples and randomly drop twits with neutral sentiment to get around 20% neutral twits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq9rLJDKslbC"
      },
      "outputs": [],
      "source": [
        "balanced = {'messages': [], 'sentiments':[]}\n",
        "\n",
        "n_neutral = sum(1 for each in sentiments if each == 2)\n",
        "N_examples = len(sentiments)\n",
        "keep_prob = (N_examples - n_neutral)/4/n_neutral\n",
        "\n",
        "for idx, sentiment in enumerate(sentiments):\n",
        "    message = filtered[idx]\n",
        "    if len(message) == 0:\n",
        "        # skip this message because it has length zero\n",
        "        continue\n",
        "    elif sentiment != 2 or random.random() < keep_prob:\n",
        "        balanced['messages'].append(message)\n",
        "        balanced['sentiments'].append(sentiment) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J45HohJAslbD",
        "outputId": "ae6882dd-5f5b-4b74-bfe4-83969b52d3b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.19510884980164975"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_neutral = sum(1 for each in balanced['sentiments'] if each == 2)\n",
        "N_examples = len(balanced['sentiments'])\n",
        "n_neutral/N_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckKjbImOslbD"
      },
      "source": [
        "Convert tokens into integer ids which can be passed to the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGTK2RZFslbD"
      },
      "outputs": [],
      "source": [
        "token_ids = [[vocab[word] for word in message] for message in balanced['messages']]\n",
        "sentiments = balanced['sentiments']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCJ0Rz2xslbE"
      },
      "source": [
        "## Build Neural Network LSTM\n",
        "\n",
        "### Implement the text classifier\n",
        "\n",
        "Softmax instead of sigmoid is used because the output of NN is not a binary. In the network, sentiment scores have 5 possible outcomes. An outcome with the highest probability is the best choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tlzsWKsslbE"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, lstm_size, output_size, lstm_layers=1, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "            vocab_size : The vocabulary size.\n",
        "            embed_size : The embedding layer size.\n",
        "            lstm_size : The LSTM layer size.\n",
        "            output_size : The output size.\n",
        "            lstm_layers : The number of LSTM layers.\n",
        "            dropout : The dropout probability.\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.lstm_size = lstm_size\n",
        "        self.output_size = output_size\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # TODO Implement\n",
        "\n",
        "        # Setup embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
        "        \n",
        "        # Setup additional layers\n",
        "        self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers,\n",
        "                            dropout = dropout, batch_first = False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_size, output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\" \n",
        "        Initializes hidden state\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "            batch_size : The size of batches.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "            hidden_state\n",
        "            \n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO Implement \n",
        "        \n",
        "        \n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        hidden_state = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
        "                        weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
        "        return hidden_state\n",
        "\n",
        "\n",
        "    def forward(self, nn_input, hidden_state):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on nn_input.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "            nn_input : The batch of input to the NN.\n",
        "            hidden_state : The LSTM hidden state.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "            logps: log softmax output\n",
        "            hidden_state: The new hidden state.\n",
        "\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO Implement \n",
        "        #print(nn_input)\n",
        "        batch_size = nn_input.size(1)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        nn_input_long = nn_input.long()\n",
        "        embeds = self.embedding(nn_input_long)\n",
        "        lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        #lstm_out = lstm_out.contiguous().view(-1, self.lstm_size)\n",
        "        lstm_out = lstm_out[-1,:,:]  \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # softmax function\n",
        "        logps = self.log_softmax(out)\n",
        "        \n",
        "        #logps = logps.view(-1, batch_size, self.output_size)\n",
        "        #logps = logps[-1,:,:] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "\n",
        "        return logps, hidden_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoLAsP7HslbF"
      },
      "source": [
        "### View Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzYEPeKUslbF",
        "outputId": "e9677cd1-b8c1-4c28-d9b3-556b8448f990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-1.7876, -1.4405, -1.5648, -1.7997, -1.5081],\n",
            "        [-1.7840, -1.4768, -1.5033, -1.7929, -1.5381],\n",
            "        [-1.7983, -1.4138, -1.5963, -1.8173, -1.4868],\n",
            "        [-1.7808, -1.4354, -1.5691, -1.8099, -1.5071]])\n",
            "torch.Size([4, 5])\n"
          ]
        }
      ],
      "source": [
        "model = TextClassifier(len(vocab), 10, 6, 5, dropout=0.1, lstm_layers=2)\n",
        "model.embedding.weight.data.uniform_(-1, 1)\n",
        "input = torch.randint(0, 1000, (5, 4), dtype=torch.int64)\n",
        "hidden = model.init_hidden(4)\n",
        "\n",
        "logps, _ = model.forward(input, hidden)\n",
        "print(logps)\n",
        "print(logps.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wDKrjWUslbF"
      },
      "source": [
        "## Training\n",
        "### DataLoaders and Batching\n",
        "Now we'll build a generator that we can use to loop through our data. It'll be more efficient if we can pass our sequences in as batches. Our input tensors should look like `(sequence_length, batch_size)`. So if our sequences are 40 tokens long and we pass in 25 sequences, then we'd have an input size of `(40, 25)`.\n",
        "\n",
        "If we set our sequence length to 40, for messages with fewer than 40 tokens, we will pad the empty spots with zeros. We should be sure to **left** pad so that the RNN starts from nothing before going through the data. If the message has 20 tokens, then the first 20 spots of our 40 long sequence will be 0. If a message has more than 40 tokens, we'll just keep the first 40 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZOL8oIYslbG"
      },
      "outputs": [],
      "source": [
        "def dataloader(messages, labels, sequence_length=30, batch_size=32, shuffle=False):\n",
        "    \"\"\" \n",
        "    Build a dataloader.\n",
        "    \"\"\"\n",
        "    if shuffle:\n",
        "        indices = list(range(len(messages)))\n",
        "        random.shuffle(indices)\n",
        "        messages = [messages[idx] for idx in indices]\n",
        "        labels = [labels[idx] for idx in indices]\n",
        "\n",
        "    total_sequences = len(messages)\n",
        "\n",
        "    for ii in range(0, total_sequences, batch_size):\n",
        "        batch_messages = messages[ii: ii+batch_size]\n",
        "        \n",
        "        # First initialize a tensor of all zeros\n",
        "        batch = torch.zeros((sequence_length, len(batch_messages)), dtype=torch.int64)\n",
        "        for batch_num, tokens in enumerate(batch_messages):\n",
        "            token_tensor = torch.tensor(tokens)\n",
        "            # Left pad!\n",
        "            start_idx = max(sequence_length - len(token_tensor), 0)\n",
        "            batch[start_idx:, batch_num] = token_tensor[:sequence_length]\n",
        "        \n",
        "        label_tensor = torch.tensor(labels[ii: ii+len(batch_messages)])\n",
        "        \n",
        "        yield batch, label_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZdzg8GtslbG"
      },
      "source": [
        "### Training and  Validation\n",
        "With our data in nice shape, we'll split it into training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvWnXaRzslbG",
        "outputId": "dc621fa9-0ae5-47ea-a647-5f96e9baac12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1859, 3050, 5107, 465, 1514, 484, 1452, 4623, 350, 366, 230, 108, 454, 1276], [426, 503, 728, 752, 38027, 1488, 5611, 46807], [134, 57, 654, 80], [84, 336, 1444, 758], [209, 1382, 1986, 2805, 1761, 358, 1025, 3251, 1184, 59], [41, 1332, 1243, 4239, 144], [7359, 126, 697, 1774, 1369, 1434, 6216, 7, 714, 1416, 355, 454, 5040, 149, 263], [4446, 7111, 3189, 488, 1284], [3453, 135], [6979, 641, 5765, 4506]]\n",
            "[0, 4, 4, 0, 3, 1, 3, 3, 3, 2]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Split data into training and validation datasets. Use an appropriate split size.\n",
        "The features are the `token_ids` and the labels are the `sentiments`.\n",
        "\"\"\"   \n",
        "\n",
        "# TODO Implement \n",
        "split_frac = 0.8\n",
        "split_idx = int(len(token_ids)*split_frac)\n",
        "\n",
        "train_features = token_ids[:split_idx]\n",
        "valid_features = token_ids[split_idx:]\n",
        "train_labels = sentiments[:split_idx]\n",
        "valid_labels = sentiments[split_idx:]\n",
        "\n",
        "print(valid_features[:10])\n",
        "print(valid_labels[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVGfCYVFslbH",
        "outputId": "be4c55d6-579d-4685-ade4-8df717628c72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 5])\n"
          ]
        }
      ],
      "source": [
        "text_batch, labels = next(iter(dataloader(train_features, train_labels, sequence_length=20, batch_size=64)))\n",
        "model = TextClassifier(len(vocab)+1, 200, 128, 5, dropout=0.)\n",
        "hidden = model.init_hidden(64)\n",
        "logps, hidden = model.forward(text_batch, hidden)\n",
        "print(logps.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1NGPYPxslbH"
      },
      "source": [
        "### Training\n",
        "It's time to train the neural network!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZK06CRmslbH",
        "outputId": "896e69cb-e084-4b4a-c078-b0af4dc813d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextClassifier(\n",
              "  (embedding): Embedding(47982, 1024)\n",
              "  (lstm): LSTM(1024, 512, num_layers=2, dropout=0.2)\n",
              "  (dropout): Dropout(p=0.2)\n",
              "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
              "  (log_softmax): LogSoftmax()\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TextClassifier(len(vocab)+1, 1024, 512, 5, lstm_layers=2, dropout=0.2)\n",
        "model.embedding.weight.data.uniform_(-1, 1)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C5D68-_slbI",
        "outputId": "a575e433-5ece-4ec9-d609-df216d482dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Epoch: 1/4... Step: 100... Loss: 1.049599... Val Loss: 1.083315 Test accuracy: 0.573\n",
            "Epoch: 1/4... Step: 200... Loss: 0.891162... Val Loss: 0.952108 Test accuracy: 0.620\n",
            "Epoch: 1/4... Step: 300... Loss: 0.895416... Val Loss: 0.922335 Test accuracy: 0.637\n",
            "Epoch: 1/4... Step: 400... Loss: 0.862422... Val Loss: 0.877372 Test accuracy: 0.662\n",
            "Epoch: 1/4... Step: 500... Loss: 0.886698... Val Loss: 0.875703 Test accuracy: 0.660\n",
            "Epoch: 1/4... Step: 600... Loss: 0.804387... Val Loss: 0.848205 Test accuracy: 0.672\n",
            "Epoch: 1/4... Step: 700... Loss: 0.840885... Val Loss: 0.839681 Test accuracy: 0.675\n",
            "Epoch: 1/4... Step: 800... Loss: 0.757674... Val Loss: 0.823805 Test accuracy: 0.682\n",
            "Epoch: 1/4... Step: 900... Loss: 0.759832... Val Loss: 0.807475 Test accuracy: 0.689\n",
            "Epoch: 1/4... Step: 1000... Loss: 0.805853... Val Loss: 0.817683 Test accuracy: 0.683\n",
            "Epoch: 1/4... Step: 1100... Loss: 0.757313... Val Loss: 0.806333 Test accuracy: 0.688\n",
            "Epoch: 1/4... Step: 1200... Loss: 0.792271... Val Loss: 0.800500 Test accuracy: 0.691\n",
            "Epoch: 1/4... Step: 1300... Loss: 0.707388... Val Loss: 0.792701 Test accuracy: 0.694\n",
            "Epoch: 1/4... Step: 1400... Loss: 0.727930... Val Loss: 0.793771 Test accuracy: 0.692\n",
            "Epoch: 1/4... Step: 1500... Loss: 0.775822... Val Loss: 0.788636 Test accuracy: 0.698\n",
            "Epoch: 1/4... Step: 1600... Loss: 0.790718... Val Loss: 0.795415 Test accuracy: 0.691\n",
            "Starting epoch 2\n",
            "Epoch: 2/4... Step: 100... Loss: 0.630868... Val Loss: 0.803680 Test accuracy: 0.690\n",
            "Epoch: 2/4... Step: 200... Loss: 0.758039... Val Loss: 0.803899 Test accuracy: 0.688\n",
            "Epoch: 2/4... Step: 300... Loss: 0.678720... Val Loss: 0.798398 Test accuracy: 0.691\n",
            "Epoch: 2/4... Step: 400... Loss: 0.691704... Val Loss: 0.792781 Test accuracy: 0.694\n",
            "Epoch: 2/4... Step: 500... Loss: 0.648244... Val Loss: 0.787469 Test accuracy: 0.698\n",
            "Epoch: 2/4... Step: 600... Loss: 0.676736... Val Loss: 0.789045 Test accuracy: 0.696\n",
            "Epoch: 2/4... Step: 700... Loss: 0.713963... Val Loss: 0.787216 Test accuracy: 0.698\n",
            "Epoch: 2/4... Step: 800... Loss: 0.709763... Val Loss: 0.787847 Test accuracy: 0.700\n",
            "Epoch: 2/4... Step: 900... Loss: 0.775308... Val Loss: 0.779219 Test accuracy: 0.702\n",
            "Epoch: 2/4... Step: 1000... Loss: 0.720633... Val Loss: 0.786036 Test accuracy: 0.702\n",
            "Epoch: 2/4... Step: 1100... Loss: 0.716576... Val Loss: 0.769556 Test accuracy: 0.707\n",
            "Epoch: 2/4... Step: 1200... Loss: 0.659283... Val Loss: 0.779136 Test accuracy: 0.700\n",
            "Epoch: 2/4... Step: 1300... Loss: 0.696550... Val Loss: 0.783599 Test accuracy: 0.699\n",
            "Epoch: 2/4... Step: 1400... Loss: 0.729054... Val Loss: 0.777057 Test accuracy: 0.705\n",
            "Epoch: 2/4... Step: 1500... Loss: 0.762223... Val Loss: 0.768029 Test accuracy: 0.706\n",
            "Epoch: 2/4... Step: 1600... Loss: 0.706958... Val Loss: 0.767846 Test accuracy: 0.706\n",
            "Starting epoch 3\n",
            "Epoch: 3/4... Step: 100... Loss: 0.598577... Val Loss: 0.814617 Test accuracy: 0.698\n",
            "Epoch: 3/4... Step: 200... Loss: 0.565012... Val Loss: 0.811040 Test accuracy: 0.699\n",
            "Epoch: 3/4... Step: 300... Loss: 0.596543... Val Loss: 0.800411 Test accuracy: 0.701\n",
            "Epoch: 3/4... Step: 400... Loss: 0.677537... Val Loss: 0.801863 Test accuracy: 0.702\n",
            "Epoch: 3/4... Step: 500... Loss: 0.598226... Val Loss: 0.795240 Test accuracy: 0.702\n",
            "Epoch: 3/4... Step: 600... Loss: 0.634489... Val Loss: 0.807434 Test accuracy: 0.700\n",
            "Epoch: 3/4... Step: 700... Loss: 0.657353... Val Loss: 0.799081 Test accuracy: 0.702\n",
            "Epoch: 3/4... Step: 800... Loss: 0.625296... Val Loss: 0.798068 Test accuracy: 0.698\n",
            "Epoch: 3/4... Step: 900... Loss: 0.613782... Val Loss: 0.812259 Test accuracy: 0.695\n",
            "Epoch: 3/4... Step: 1000... Loss: 0.701977... Val Loss: 0.815646 Test accuracy: 0.695\n",
            "Epoch: 3/4... Step: 1100... Loss: 0.627163... Val Loss: 0.823241 Test accuracy: 0.695\n",
            "Epoch: 3/4... Step: 1200... Loss: 0.647973... Val Loss: 0.801778 Test accuracy: 0.695\n",
            "Epoch: 3/4... Step: 1300... Loss: 0.562137... Val Loss: 0.789492 Test accuracy: 0.703\n",
            "Epoch: 3/4... Step: 1400... Loss: 0.664249... Val Loss: 0.804806 Test accuracy: 0.697\n",
            "Epoch: 3/4... Step: 1500... Loss: 0.647442... Val Loss: 0.793799 Test accuracy: 0.699\n",
            "Epoch: 3/4... Step: 1600... Loss: 0.620628... Val Loss: 0.792858 Test accuracy: 0.701\n",
            "Starting epoch 4\n",
            "Epoch: 4/4... Step: 100... Loss: 0.534464... Val Loss: 0.870190 Test accuracy: 0.696\n",
            "Epoch: 4/4... Step: 200... Loss: 0.478951... Val Loss: 0.882421 Test accuracy: 0.687\n",
            "Epoch: 4/4... Step: 300... Loss: 0.534338... Val Loss: 0.880773 Test accuracy: 0.683\n",
            "Epoch: 4/4... Step: 400... Loss: 0.557707... Val Loss: 0.878168 Test accuracy: 0.691\n",
            "Epoch: 4/4... Step: 500... Loss: 0.534438... Val Loss: 0.857660 Test accuracy: 0.695\n",
            "Epoch: 4/4... Step: 600... Loss: 0.544428... Val Loss: 0.861975 Test accuracy: 0.698\n",
            "Epoch: 4/4... Step: 700... Loss: 0.581466... Val Loss: 0.864151 Test accuracy: 0.696\n",
            "Epoch: 4/4... Step: 800... Loss: 0.618363... Val Loss: 0.857294 Test accuracy: 0.695\n",
            "Epoch: 4/4... Step: 900... Loss: 0.574767... Val Loss: 0.851013 Test accuracy: 0.694\n",
            "Epoch: 4/4... Step: 1000... Loss: 0.542496... Val Loss: 0.868141 Test accuracy: 0.691\n",
            "Epoch: 4/4... Step: 1100... Loss: 0.521352... Val Loss: 0.862120 Test accuracy: 0.692\n",
            "Epoch: 4/4... Step: 1200... Loss: 0.603699... Val Loss: 0.860161 Test accuracy: 0.686\n",
            "Epoch: 4/4... Step: 1300... Loss: 0.509636... Val Loss: 0.870361 Test accuracy: 0.689\n",
            "Epoch: 4/4... Step: 1400... Loss: 0.551277... Val Loss: 0.880132 Test accuracy: 0.691\n",
            "Epoch: 4/4... Step: 1500... Loss: 0.639750... Val Loss: 0.869539 Test accuracy: 0.690\n",
            "Epoch: 4/4... Step: 1600... Loss: 0.573762... Val Loss: 0.873682 Test accuracy: 0.692\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\"\"\"\n",
        "Train your model with dropout. Make sure to clip your gradients.\n",
        "Print the training loss, validation loss, and validation accuracy for every 100 steps.\n",
        "\"\"\"\n",
        "epochs = 4\n",
        "batch_size = 512\n",
        "learning_rate = 0.001\n",
        "clip = 5\n",
        "\n",
        "print_every = 100\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "model.train()\n",
        "\n",
        "sequence_length = 20\n",
        "for epoch in range(epochs):\n",
        "    print('Starting epoch {}'.format(epoch + 1))\n",
        "    \n",
        "    steps = 0\n",
        "    # initialize hidden state\n",
        "    \n",
        "    for text_batch, labels in dataloader(\n",
        "            train_features, train_labels, batch_size=batch_size, sequence_length=sequence_length, shuffle=True):\n",
        "        steps += 1\n",
        "        min_batch_size = min(batch_size,text_batch.size(1))\n",
        "        hidden = model.init_hidden(min_batch_size)\n",
        "        \n",
        "        text_batch, labels = text_batch.to(device), labels.to(device)\n",
        "        for each in hidden:\n",
        "            each.to(device)\n",
        "        \n",
        "        # TODO Implement: Train Model\n",
        "        \n",
        "        # zero accumulated gradients\n",
        "        model.zero_grad()\n",
        "        \n",
        "        # get the output from the model\n",
        "        output, h = model(text_batch, hidden)\n",
        "\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze().view(min_batch_size,-1), labels)\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if steps % print_every == 0:\n",
        "            model.eval()\n",
        "            \n",
        "            # TODO Implement: Print metrics\n",
        "            # Get validation loss\n",
        "            \n",
        "            val_losses = []\n",
        "            number_correct = 0\n",
        "            for text_batch, labels in dataloader(valid_features, valid_labels, batch_size=batch_size, \n",
        "                    sequence_length=sequence_length, shuffle=False):\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                min_batch_size = min(text_batch.size(1),batch_size)\n",
        "                val_h = model.init_hidden(min_batch_size)\n",
        "                #text_batch, labels = text_batch.cuda(), labels.cuda()\n",
        "                text_batch, labels = text_batch.to(device), labels.to(device)\n",
        "                for each in hidden:\n",
        "                    each.to(device)\n",
        "                #print(text_batch.size())\n",
        "                output, val_h = model(text_batch, val_h)\n",
        "                #print(output.size())\n",
        "                val_loss = criterion(output, labels)\n",
        "                val_losses.append(val_loss.item())\n",
        "                #print(output)\n",
        "                # convert output probabilities to predicted class (0 or 1)\n",
        "                pred = torch.argmax(torch.exp(output),dim=-1)  # rounds to the nearest integer\n",
        "                #print(pred,labels.size())\n",
        "                \n",
        "                # compare predictions to true label\n",
        "                correct_tensor = pred.eq(labels.view_as(pred))\n",
        "                train_on_gpu=torch.cuda.is_available()\n",
        "                correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    \n",
        "                number_correct += np.sum(correct)\n",
        "\n",
        "            # accuracy over all test data\n",
        "            test_acc = number_correct/len(valid_labels)\n",
        "\n",
        "            print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
        "                  \"Step: {}...\".format(steps),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Test accuracy: {:.3f}\".format(test_acc))\n",
        "            model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtUOtgtdslbI"
      },
      "source": [
        "## Making Predictions\n",
        "### Prediction \n",
        "Now implement the `predict` function to generate the prediction vector from a message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulEbwpmbslbJ"
      },
      "outputs": [],
      "source": [
        "def predict(text, model, vocab):\n",
        "    \"\"\" \n",
        "    Make a prediction on a single sentence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        text : The string to make a prediction on.\n",
        "        model : The model to use for making the prediction.\n",
        "        vocab : Dictionary for word to word ids. The key is the word and the value is the word id.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        pred : Prediction vector\n",
        "    \"\"\"    \n",
        "    \n",
        "    # TODO Implement\n",
        "    \n",
        "    tokens = preprocess(text)\n",
        "    #print(tokens)\n",
        "    # Filter non-vocab words\n",
        "    tokens_vocab = [vocab[word] for word in tokens if word in filtered_words]\n",
        "    #print(tokens_vocab)\n",
        "    tokens_tensor = torch.tensor(tokens_vocab)\n",
        "    # Adding a batch dimension\n",
        "    batch_size = 1\n",
        "    seq_length=sequence_length\n",
        "    \n",
        "    #left pad\n",
        "    features = torch.zeros((seq_length, batch_size), dtype=torch.int64)\n",
        "    features[max(-seq_length, -len(tokens_tensor)):,0] = tokens_tensor[:min(seq_length,len(tokens_tensor))]\n",
        "    #print(features)\n",
        "    # Get the NN output\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    logps, _ = model(features, hidden)\n",
        "    # Take the exponent of the NN output to get a range of 0 to 1 for each label.\n",
        "    pred = torch.exp(logps) \n",
        "    \n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMpXYmuCslbJ",
        "outputId": "c4ce719a-ba88-4c98-ee25-25c58430745c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.0003,  0.0607,  0.0125,  0.8170,  0.1094]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"Google is working on self driving cars, I'm bullish on $goog\"\n",
        "model.eval()\n",
        "model.to(\"cpu\")\n",
        "predict(text, model, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW9VaqacslbJ"
      },
      "source": [
        "### The prediction of the model is positive. The uncertainty of the prediction is 19.50%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drIPEl-FslbJ"
      },
      "source": [
        "Now we have a trained model and we can make predictions. We can use this model to track the sentiments of various stocks by predicting the sentiments of twits as they are coming in. Now we have a stream of twits. For each of those twits, pull out the stocks mentioned in them and keep track of the sentiments. Remember that in the twits, ticker symbols are encoded with a dollar sign as the first character, all caps, and 2-4 letters, like $AAPL. Ideally, you'd want to track the sentiments of the stocks in your universe and use this as a signal in your larger model(s).\n",
        "\n",
        "## Testing\n",
        "### Load the Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmSvx0suslbK"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join('..', '..', 'data', 'project_6_stocktwits', 'test_twits.json'), 'r') as f:\n",
        "    test_data = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLvY3JGAslbK"
      },
      "source": [
        "### Twit Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSuj8sw7slbK",
        "outputId": "e92c76e7-2868-4209-9a06-c24f3ec8c281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'message_body': '$JWN has moved -1.69% on 10-31. Check out the movement and peers at  https://dividendbot.com?s=JWN',\n",
              " 'timestamp': '2018-11-01T00:00:05Z'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def twit_stream():\n",
        "    for twit in test_data['data']:\n",
        "        yield twit\n",
        "\n",
        "next(twit_stream())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHSNLH7PslbK"
      },
      "source": [
        "Using the `prediction` function, let's apply it to a stream of twits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwgIP78CslbK"
      },
      "outputs": [],
      "source": [
        "def score_twits(stream, model, vocab, universe):\n",
        "    \"\"\" \n",
        "    Given a stream of twits and a universe of tickers, return sentiment scores for tickers in the universe.\n",
        "    \"\"\"\n",
        "    for twit in stream:\n",
        "\n",
        "        # Get the message text\n",
        "        text = twit['message_body']\n",
        "        symbols = re.findall('\\$[A-Z]{2,4}', text)\n",
        "        score = predict(text, model, vocab)\n",
        "\n",
        "        for symbol in symbols:\n",
        "            if symbol in universe:\n",
        "                yield {'symbol': symbol, 'score': score, 'timestamp': twit['timestamp']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6OXLeELslbL",
        "outputId": "c2ca47a3-cf37-498a-b555-aab079609f8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'symbol': '$AAPL',\n",
              " 'score': tensor([[ 0.1609,  0.0094,  0.0307,  0.0378,  0.7612]]),\n",
              " 'timestamp': '2018-11-01T00:00:18Z'}"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "universe = {'$BBRY', '$AAPL', '$AMZN', '$BABA', '$YHOO', '$LQMT', '$FB', '$GOOG', '$BBBY', '$JNUG', '$SBUX', '$MU'}\n",
        "score_stream = score_twits(twit_stream(), model, vocab, universe)\n",
        "\n",
        "next(score_stream)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16luVPZNslbL"
      },
      "source": [
        "That's it. We have successfully built a model for sentiment analysis! "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Use NLP, NN and LSTM to Analyze Stock Sentiment from Twits.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}